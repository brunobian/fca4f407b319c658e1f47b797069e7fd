# options(digits=2)
rm(list=ls()) #borro todas las variables del workspace (rm)
# setwd('/home/juank/Repos/LSA/R/')
setwd('/home/brunobian/Documents/Repos/LSA/R')
library(lme4)
library(plyr)
library(dplyr)
library(ggplot2)
library(lattice)
library(RColorBrewer)
library(reshape2)
library(corrplot)
library(gridExtra)
source('functions/loadDataSets.R')
source('functions/jk.logit.R')
source('functions/bb.logit.R')
source('functions/remef.v0.6.10.R')
figFolder = '/home/brunobian/Dropbox/Labo/reading/9 manuscritos/Predicting Predictability/Figures/Round 1/1-originales/'
##### Load DataFrames #####################################################################################################
# Load Eye data
# Esto viene de la ultima version del paper de Repeticiones.
# Copie los archivos necesarios para generarlo aca.
load("eye_data/data_without_skips_2019.Rda")
# summary(data)
# Load Cloze data
# Esto viene de Gaston, solo uso la data Cloze
# load("pred_data/dataCLOZE.Rda")
# summary(dataCLOZE)
# data$CLOZE_pred <- jk.logit(dataCLOZE$CLOZE_pred)
cloze <- read.csv("../cloze/predictability2.csv")
cloze <- bb.logit(cloze)
data  <- addCloze(data, cloze)
data  <- data[data$CLOZE_nPred >= 8,]
data  <- data[data$CLOZE_nPred_next >= 8,]
data   <- loadEyeData(data)
# Merge DataFrames
# Load LSA data and Add LSA to the Eye data
# - Las últimas 15 columnas son de LSA:
# 'Texto': nombre del texto (basta de números)
# 'Palabra Target': palabra
# 'wng (inicia en 1)': posicion absoluta en el texto
# 'Tamaño ventana': ventana, coincide con el numero del archivo
# 'Pals Ventana Con SW': palabras en la ventana que incluye stopwords
# 'Pals Ventana Sin SW': palabras en la ventana que no incluye stopwords
# 'N Pals Contenido': cantidad de palabras de contenido en la ventana que incluye stopwords (si la ventana es de 50, son 50 palabras totales, N palabras de contenido)
# 'D resultante conSW': distancia de la palabra target a la resultante de la ventana con SW
# 'D resultante sinSW': distancia de la palabra target a la resultante de la ventana sin SW
# 'D promedio conSW': promedio de las  entre la palabra target y cada una de las palabras de la ventana con SW
# 'D promedio sinSW': promedio de las  entre la palabra target y cada una de las palabras de la ventana sin SW
# 'D minima conSW': distancia mínima dentro de la ventana
# 'D minima sinSW': distancia mínima dentro de la ventana
# 'd_w2v_sinSW': distancia de w2v en ventana sin SW
# 'd_w2v_conSW': distancia de w2v en ventana sin SW
# 'p_cercanas_sinSW': proporcion de palabras cercanas en LSA en ventana sin SW
# 'p_cercanas_conSW': proporcion de palabras cercanas en LSA  en ventana con SW
# A cada elemento de data hay que asignarle los nuevos regresores que me interesen.
ventanas = c('002','003','004','005',
'006','007','008','009',
'010','011','012',
'015','017','020','030','040','050',
'060','100','150')
# ventanas = c('005','015','025','050',
#              '100','150')
for (v in ventanas) {
file <- paste0("../distancias/usandoSW_FastText_corpus/ventana",v, ".csv")
LSA  <- read.csv(file, comment.char = "") # Tiene todas, aun las palabras que no se repitieron
LSA$d_w2v_sinSW  <- as.numeric(LSA$d_w2v_sinSW)
data <- addLsaW2vToData(data, LSA, v)
}
ngrams4 <- read.csv('~/Documents/Repos/ngrams/TABLE_complete2')
data <- addNgramsToData(data, ngrams4)
data <- generateNgramCache(data)
# Elimino pals con NA
dataAll <- data
# load(file="FullData.rda")
dataAll -> data
data[data==-10] <- NA
tmp <- c()
for (i in colnames(data)){
tmp <- c(tmp,sum(is.na(data[,i])))
}
hist(tmp,100)
# Elimino las columnas que me generan muchos NA
colnames(data)[tmp>20000]
data <- data[,colnames(data)[tmp<20000]]
# Elimino todas las filas con al menos un NA,
# para que todos los modelos corran con los mismos datos
data <- data[complete.cases(data),]
save(file="FullData.rda",list="dataAll")
save(file="DataFilt.rda",list="data")
setwd("~/Documents/Repos/fca4f407b319c658e1f47b797069e7fd")
# options(digits=2)
rm(list=ls()) #borro todas las variables del workspace (rm)
library(lme4)
library(plyr)
library(dplyr)
library(ggplot2)
library(lattice)
library(RColorBrewer)
library(reshape2)
library(corrplot)
library(gridExtra)
##### Load filtered DataFrame  #####################################################################################################
load(file="data.rda")
#####[Fig 1: Pred descriptivo]#####################################################################################################
# Correlación nRep - Pred
dataRep  <- data[(data$nREP < 9),]
tableRep <- ddply(dataRep, .(nREP), summarise,
N=length(CLOZE_pred),
M=mean(CLOZE_pred),
SD=sd(CLOZE_pred),
SE=SD/sqrt(N) )
ggplot(tableRep, aes(x = nREP, y = M, colour = 1)) +
geom_errorbar(aes(ymin = M-SE, ymax = M+SE)) +
geom_point() + geom_line()+
theme(legend.position = "none")+
xlab('Repetition number') +
ylab('Logit cloze-Predictability')
# Correlación log(freq) - Pred
dataFreq  <- data
dataFreq$freq_round <- round(data$freq_noncentered*4)/4
tableFreq <- ddply(dataFreq, .(freq_round), summarise,
N=length(CLOZE_pred),
M=mean(CLOZE_pred),
SD=sd(CLOZE_pred),
SE=SD/sqrt(N) )
ggplot(tableFreq, aes(x = freq_round, y = M, colour = 1)) +
geom_errorbar(aes(ymin = M-SE, ymax = M+SE)) +
geom_point() + geom_line()+
theme(legend.position = "none")+
xlab('Log10 Frequency') +
ylab('Logit cloze-Predictability')
# Correlación log(length) - Pred
dataLength  <- data[(data$invlength_noncentered > 0.07),]
tableLength <- ddply(dataLength, .(invlength_noncentered), summarise,
N=length(CLOZE_pred),
M=mean(CLOZE_pred),
SD=sd(CLOZE_pred),
SE=SD/sqrt(N) )
ggplot(tableLength, aes(x = invlength_noncentered, y = M, colour = 1)) +
geom_errorbar(aes(ymin = M-SE, ymax = M+SE)) +
geom_point() +
geom_line()+
theme(legend.position = "none")+
xlab('Inverse Length') +
ylab('Logit cloze-Predictability')
# Correlación rps - Pred
dataRps <- data
dataRps$rps_round <- round(dataRps$rps_noncentered*5)/5
tableRps <- ddply(dataRps, .(rps_round), summarise,
N=length(CLOZE_pred),
M=mean(CLOZE_pred),
SD=sd(CLOZE_pred),
SE=SD/sqrt(N) )
ggplot(tableRps, aes(x = rps_round, y = M, colour = 1)) +
geom_errorbar(aes(ymin = M-SE, ymax = M+SE)) +
geom_point() +
geom_line()+
theme(legend.position = "none")+
xlab('Relative Position in Sentence') +
ylab('Logit cloze-Predictability')
#####[Fig S1: optimizo ngram]###################################################################################################
# Optimizo N
enes=1:7
correlation = c()
for (n in enes){
name=paste0('X',n,'.gram')
correlation = c(correlation,cor(ngrams4$cloze_predictor, ngrams4[name]))
}
ngrams4 <- read.csv('ngram4')
ngrams4
for (n in enes){
name=paste0('X',n,'.gram')
correlation = c(correlation,cor(ngrams4$cloze_predictor, ngrams4[name]))
}
df = data.frame(enes, correlation)
df
ggplot(df, aes(x = enes, y = correlation, colour = 1)) +
geom_point() + geom_line()+
theme(legend.position = "none")+
xlab('N') +
ylab('Correlation with predictability')
load(file='Optim_delta_lambda.rda')
AICm <- dcast(data = df,formula = lambdas~deltas,value.var = "aicngram")
rownames(AICm) <- AICm[,1]
AICm[,1] <- NULL
AICm <- data.matrix(AICm)
levelplot(AICm[,2:dim(AICm)[2]],
xlab = 'lambda',
ylab = 'delta',
main = 'Model AIC',
col.regions = colorRampPalette(brewer.pal(9,"Reds"))(16))
inds = which(AICm == min(AICm), arr.ind = T)
print(paste('delta:', colnames(AICm)[inds[2]], 'lambda:', rownames(AICm)[inds[1]]))
tvalm <- dcast(data = df,formula = lambdas~deltas,value.var = "tvalngram")
rownames(tvalm) <- tvalm[,1]
tvalm[,1] <- NULL
tvalm <- data.matrix(tvalm)
a <- levelplot(tvalm[,2:dim(tvalm)[2]],
xlab = 'lambda',
ylab = 'delta',
main = 'Model tval',
col.regions = colorRampPalette(brewer.pal(9,"Reds"))(16))
inds = which(AICm == min(AICm), arr.ind = T)
print(paste('delta:', colnames(AICm)[inds[2]], 'lambda:', rownames(AICm)[inds[1]]))
#####[Fig S2: analizo Embeddings]####################################################################################################
# Corro el modelo base para comparar
M <- lmer(logFPRT ~ Nlaunchsite + invlength*freq +
rpl + rpt + rps +
(1|sujid) + (1|textid) + (1|wordid), data = data)
#####[Table 1: modelos en N]#####################################################################################################
# M0 Baseline model
M0_N <- lmer(logFPRT ~ Nlaunchsite + invlength*freq +
rpl + rpt + rps +
(1|sujid) + (1|textid) + (1|wordid), data = data, REML = FALSE)
#####[Fig S2: analizo Embeddings]####################################################################################################
# Corro el modelo base para comparar
M <- lmer(logFPRT ~ Nlaunchsite + invlength*freq +
rpl + rpt + rps +
(1|sujid) + (1|textid) + (1|wordid), data = data)
#####[Fig S2: analizo Embeddings]####################################################################################################
# Corro el modelo base para comparar
M <- lmer(logFPRT ~ Nlaunchsite + invlength*freq +
rpl + rpt + rps +
(1|sujid) + (1|textid) + (1|wordid), data = data, REML = FALSE)
aicBL  <- extractAIC(M)[2]
tvalBL <- coef(summary(M))[,"t value"]['este']
# Inicializo vars
aicResult  <- tvalResult <- aicProm <- tvalProm <- c()
aicMin <- tvalMin <- aicw2v <- tvalw2v <- c()
corResult <- corProm <- corMin <- corw2v <- c()
aicpCercanas  <- tvalpCercanas <- corpCercanas <- c()
aicFT_w <- tvalFT_w  <- corFT_w <- c()
aicFT_c <- tvalFT_c  <- corFT_c <- c()
source('loadDataSets.R')
# Corro modelos para todas las variables
n1 <- FALSE
for (v in ventanas) {
# LSA pCercanas
str = 'pCercanas'
model = modelo(str, v, data, n1)
aicpCercanas  = c(aicpCercanas,  model[1])
tvalpCercanas = c(tvalpCercanas, model[2])
corpCercanas  = c(corpCercanas,  model[3])
# LSA Resultante
str = 'resultante'
model = modelo(str, v, data, n1)
aicResult  = c(aicResult,  model[1])
tvalResult = c(tvalResult, model[2])
corResult  = c(corResult,  model[3])
# LSA promedio
str = "promedio"
model = modelo(str, v, data, n1)
aicProm  = c(aicProm, model[1])
tvalProm = c(tvalProm, model[2])
corProm  = c(corProm,  model[3])
# LSA minima
str = "minima"
model = modelo(str, v, data, n1)
aicMin  = c(aicMin, model[1])
tvalMin = c(tvalMin, model[2])
corMin  = c(corMin,  model[3])
# w2v promedio
str = 'w2v'
model = modelo(str, v, data, n1)
aicw2v  = c(aicw2v, model[1])
tvalw2v = c(tvalw2v, model[2])
corw2v  = c(corw2v,  model[3])
# FT wikipedia
str = 'FT_w'
model = modelo(str, v, data, n1)
aicFT_w  = c(aicFT_w, model[1])
tvalFT_w = c(tvalFT_w, model[2])
corFT_w  = c(corFT_w,  model[3])
# FT corpus
str = 'FT_c'
model = modelo(str, v, data, n1)
aicFT_c  = c(aicFT_c, model[1])
tvalFT_c = c(tvalFT_c, model[2])
corFT_c  = c(corFT_c,  model[3])
}
ventanas = c('002','003','004','005',
'006','007','008','009',
'010','011','012',
'015','017','020','030','040','050',
'060','100','150')
for (v in ventanas) {
# LSA pCercanas
str = 'pCercanas'
model = modelo(str, v, data, n1)
aicpCercanas  = c(aicpCercanas,  model[1])
tvalpCercanas = c(tvalpCercanas, model[2])
corpCercanas  = c(corpCercanas,  model[3])
# LSA Resultante
str = 'resultante'
model = modelo(str, v, data, n1)
aicResult  = c(aicResult,  model[1])
tvalResult = c(tvalResult, model[2])
corResult  = c(corResult,  model[3])
# LSA promedio
str = "promedio"
model = modelo(str, v, data, n1)
aicProm  = c(aicProm, model[1])
tvalProm = c(tvalProm, model[2])
corProm  = c(corProm,  model[3])
# LSA minima
str = "minima"
model = modelo(str, v, data, n1)
aicMin  = c(aicMin, model[1])
tvalMin = c(tvalMin, model[2])
corMin  = c(corMin,  model[3])
# w2v promedio
str = 'w2v'
model = modelo(str, v, data, n1)
aicw2v  = c(aicw2v, model[1])
tvalw2v = c(tvalw2v, model[2])
corw2v  = c(corw2v,  model[3])
# FT wikipedia
str = 'FT_w'
model = modelo(str, v, data, n1)
aicFT_w  = c(aicFT_w, model[1])
tvalFT_w = c(tvalFT_w, model[2])
corFT_w  = c(corFT_w,  model[3])
# FT corpus
str = 'FT_c'
model = modelo(str, v, data, n1)
aicFT_c  = c(aicFT_c, model[1])
tvalFT_c = c(tvalFT_c, model[2])
corFT_c  = c(corFT_c,  model[3])
}
source('modelo.R')
# Corro modelos para todas las variables
n1 <- FALSE
for (v in ventanas) {
# LSA pCercanas
str = 'pCercanas'
model = modelo(str, v, data, n1)
aicpCercanas  = c(aicpCercanas,  model[1])
tvalpCercanas = c(tvalpCercanas, model[2])
corpCercanas  = c(corpCercanas,  model[3])
# LSA Resultante
str = 'resultante'
model = modelo(str, v, data, n1)
aicResult  = c(aicResult,  model[1])
tvalResult = c(tvalResult, model[2])
corResult  = c(corResult,  model[3])
# LSA promedio
str = "promedio"
model = modelo(str, v, data, n1)
aicProm  = c(aicProm, model[1])
tvalProm = c(tvalProm, model[2])
corProm  = c(corProm,  model[3])
# LSA minima
str = "minima"
model = modelo(str, v, data, n1)
aicMin  = c(aicMin, model[1])
tvalMin = c(tvalMin, model[2])
corMin  = c(corMin,  model[3])
# w2v promedio
str = 'w2v'
model = modelo(str, v, data, n1)
aicw2v  = c(aicw2v, model[1])
tvalw2v = c(tvalw2v, model[2])
corw2v  = c(corw2v,  model[3])
# FT wikipedia
str = 'FT_w'
model = modelo(str, v, data, n1)
aicFT_w  = c(aicFT_w, model[1])
tvalFT_w = c(tvalFT_w, model[2])
corFT_w  = c(corFT_w,  model[3])
# FT corpus
str = 'FT_c'
model = modelo(str, v, data, n1)
aicFT_c  = c(aicFT_c, model[1])
tvalFT_c = c(tvalFT_c, model[2])
corFT_c  = c(corFT_c,  model[3])
}
